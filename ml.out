\BOOKMARK [0][-]{chapter*.2}{Introduction}{}% 1
\BOOKMARK [1][-]{section.0.1}{notation}{chapter*.2}% 2
\BOOKMARK [0][-]{chapter.1}{Logistic Regression as a neural network}{}% 3
\BOOKMARK [1][-]{section.1.1}{definitions}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.2}{cost function}{chapter.1}% 5
\BOOKMARK [1][-]{section.1.3}{Gradient Descent}{chapter.1}% 6
\BOOKMARK [1][-]{section.1.4}{Model training}{chapter.1}% 7
\BOOKMARK [1][-]{section.1.5}{Forward Propagation}{chapter.1}% 8
\BOOKMARK [2][-]{subsection.1.5.1}{Activation Functions}{section.1.5}% 9
\BOOKMARK [1][-]{section.1.6}{Backward Propagation}{chapter.1}% 10
\BOOKMARK [1][-]{section.1.7}{Update parameters}{chapter.1}% 11
\BOOKMARK [1][-]{section.1.8}{Summary}{chapter.1}% 12
\BOOKMARK [1][-]{section.1.9}{Logistic Regression in Python}{chapter.1}% 13
\BOOKMARK [1][-]{section.1.10}{References}{chapter.1}% 14
\BOOKMARK [0][-]{chapter.2}{Neural Networks}{}% 15
\BOOKMARK [1][-]{section.2.1}{Lingua franca}{chapter.2}% 16
\BOOKMARK [1][-]{section.2.2}{Model training}{chapter.2}% 17
\BOOKMARK [1][-]{section.2.3}{Parameter initialization}{chapter.2}% 18
\BOOKMARK [1][-]{section.2.4}{Forward Propagation}{chapter.2}% 19
\BOOKMARK [1][-]{section.2.5}{Backward Propagation}{chapter.2}% 20
\BOOKMARK [1][-]{section.2.6}{Summary}{chapter.2}% 21
\BOOKMARK [1][-]{section.2.7}{Deep neural networks in Python}{chapter.2}% 22
\BOOKMARK [0][-]{chapter.3}{Neural Networks hyperparameters}{}% 23
\BOOKMARK [1][-]{subsection.3.0.1}{training}{chapter.3}% 24
\BOOKMARK [2][-]{subsection.3.0.2}{bias-variance trade-off}{subsection.3.0.1}% 25
\BOOKMARK [2][-]{subsection.3.0.3}{recipes for high-bias, high-variance}{subsection.3.0.1}% 26
\BOOKMARK [2][-]{subsection.3.0.4}{Regularization \(weight decay\)}{subsection.3.0.1}% 27
\BOOKMARK [2][-]{subsection.3.0.5}{Inverted Dropout Regularization}{subsection.3.0.1}% 28
\BOOKMARK [2][-]{subsection.3.0.6}{Input Normalization}{subsection.3.0.1}% 29
\BOOKMARK [2][-]{subsection.3.0.7}{Vanishing/Exploding gradients}{subsection.3.0.1}% 30
\BOOKMARK [2][-]{subsection.3.0.8}{gradient checking}{subsection.3.0.1}% 31
\BOOKMARK [1][-]{section.3.1}{Optimization}{chapter.3}% 32
\BOOKMARK [2][-]{subsection.3.1.1}{stochastic, mini-batch, and batch gradient descent}{section.3.1}% 33
\BOOKMARK [1][-]{section.3.2}{Gradient descent with momentum}{chapter.3}% 34
\BOOKMARK [1][-]{section.3.3}{RMSprob}{chapter.3}% 35
\BOOKMARK [1][-]{section.3.4}{Adaptive Momentum estimation \(Adam\)}{chapter.3}% 36
\BOOKMARK [2][-]{subsection.3.4.1}{Hyperparameters}{section.3.4}% 37
\BOOKMARK [1][-]{section.3.5}{Learning rate decay}{chapter.3}% 38
\BOOKMARK [1][-]{section.3.6}{Tuning parameters}{chapter.3}% 39
\BOOKMARK [2][-]{subsection.3.6.1}{coarse to fine search}{section.3.6}% 40
\BOOKMARK [2][-]{subsection.3.6.2}{panda vs caviar training approaches}{section.3.6}% 41
\BOOKMARK [1][-]{section.3.7}{Batch Normalization}{chapter.3}% 42
\BOOKMARK [2][-]{subsection.3.7.1}{Covariate Shift}{section.3.7}% 43
\BOOKMARK [2][-]{subsection.3.7.2}{Batch normalization as a regularization technique}{section.3.7}% 44
\BOOKMARK [2][-]{subsection.3.7.3}{Batch normalization on test sets}{section.3.7}% 45
\BOOKMARK [1][-]{section.3.8}{Multi-class classification}{chapter.3}% 46
\BOOKMARK [2][-]{subsection.3.8.1}{Softmax activation}{section.3.8}% 47
\BOOKMARK [0][-]{chapter.4}{Structuring machine learning}{}% 48
\BOOKMARK [1][-]{section.4.1}{Machine learning strategy}{chapter.4}% 49
\BOOKMARK [1][-]{section.4.2}{orthogonality}{chapter.4}% 50
\BOOKMARK [1][-]{section.4.3}{Set up your Goal}{chapter.4}% 51
\BOOKMARK [2][-]{subsection.4.3.1}{Evaluation metric}{section.4.3}% 52
\BOOKMARK [2][-]{subsection.4.3.2}{Precision vs accuracy}{section.4.3}% 53
\BOOKMARK [2][-]{subsection.4.3.3}{F1-score}{section.4.3}% 54
\BOOKMARK [2][-]{subsection.4.3.4}{Satisfying-Optimizing metric}{section.4.3}% 55
\BOOKMARK [1][-]{section.4.4}{train/dev/test sets}{chapter.4}% 56
\BOOKMARK [2][-]{subsection.4.4.1}{dev/test metric}{section.4.4}% 57
\BOOKMARK [1][-]{section.4.5}{Human-level performance}{chapter.4}% 58
\BOOKMARK [2][-]{subsection.4.5.1}{human\(bayes\) error}{section.4.5}% 59
\BOOKMARK [1][-]{section.4.6}{Error analysis}{chapter.4}% 60
\BOOKMARK [2][-]{subsection.4.6.1}{Mislabeled data}{section.4.6}% 61
\BOOKMARK [1][-]{section.4.7}{Mismatched training dev/test sets}{chapter.4}% 62
\BOOKMARK [2][-]{subsection.4.7.1}{Addressing data mismatch}{section.4.7}% 63
\BOOKMARK [2][-]{subsection.4.7.2}{Artificial data synthesis}{section.4.7}% 64
\BOOKMARK [1][-]{section.4.8}{Multiple tasks learning}{chapter.4}% 65
\BOOKMARK [2][-]{subsection.4.8.1}{Transfer learning}{section.4.8}% 66
\BOOKMARK [2][-]{subsection.4.8.2}{Multitask learning}{section.4.8}% 67
\BOOKMARK [1][-]{section.4.9}{End-to-End Machine Learning}{chapter.4}% 68
\BOOKMARK [0][-]{chapter.5}{Natural language processing}{}% 69
\BOOKMARK [1][-]{section.5.1}{pre-processing}{chapter.5}% 70
\BOOKMARK [1][-]{section.5.2}{Example: positive, negative classifier}{chapter.5}% 71
\BOOKMARK [1][-]{section.5.3}{Logistic regression classifier}{chapter.5}% 72
\BOOKMARK [1][-]{section.5.4}{Naive Bayes classifier}{chapter.5}% 73
\BOOKMARK [1][-]{section.5.5}{costine similaritis}{chapter.5}% 74
\BOOKMARK [2][-]{subsection.5.5.1}{Euclidean distance}{section.5.5}% 75
\BOOKMARK [1][-]{section.5.6}{Principle Component Analysis \(PCA\)}{chapter.5}% 76
\BOOKMARK [1][-]{section.5.7}{Machine Translation}{chapter.5}% 77
\BOOKMARK [2][-]{subsection.5.7.1}{Loss function L}{section.5.7}% 78
\BOOKMARK [2][-]{subsection.5.7.2}{gradient descent}{section.5.7}% 79
\BOOKMARK [2][-]{subsection.5.7.3}{fixed number of iterations}{section.5.7}% 80
\BOOKMARK [2][-]{subsection.5.7.4}{k-Nearest neighbors algorithm}{section.5.7}% 81
\BOOKMARK [2][-]{subsection.5.7.5}{Searching for the translation embedding}{section.5.7}% 82
\BOOKMARK [2][-]{subsection.5.7.6}{LSH and document search}{section.5.7}% 83
\BOOKMARK [2][-]{subsection.5.7.7}{Bag-of-words \(BOW\) document models}{section.5.7}% 84
\BOOKMARK [2][-]{subsection.5.7.8}{ Choosing the number of planes}{section.5.7}% 85
\BOOKMARK [2][-]{subsection.5.7.9}{ Getting the hash number for a vector}{section.5.7}% 86
\BOOKMARK [1][-]{section.5.8}{Probabilistic model of pronounciation and spelling}{chapter.5}% 87
\BOOKMARK [2][-]{subsection.5.8.1}{auto-correction}{section.5.8}% 88
\BOOKMARK [2][-]{subsection.5.8.2}{Bayesian inference model}{section.5.8}% 89
\BOOKMARK [2][-]{subsection.5.8.3}{Minimum edit distance}{section.5.8}% 90
\BOOKMARK [1][-]{section.5.9}{Grammar Weighted Automata}{chapter.5}% 91
\BOOKMARK [2][-]{subsection.5.9.1}{Markov chain}{section.5.9}% 92
\BOOKMARK [2][-]{subsection.5.9.2}{Hidden Markov Models HMMs}{section.5.9}% 93
\BOOKMARK [2][-]{subsection.5.9.3}{Viterbi Algorithm}{section.5.9}% 94
\BOOKMARK [2][-]{subsection.5.9.4}{Part of Speech tagging \(POS\)}{section.5.9}% 95
\BOOKMARK [1][-]{section.5.10}{N-grams}{chapter.5}% 96
\BOOKMARK [2][-]{subsection.5.10.1}{Smoothing}{section.5.10}% 97
\BOOKMARK [2][-]{subsection.5.10.2}{Back-off}{section.5.10}% 98
\BOOKMARK [2][-]{subsection.5.10.3}{Interpolation}{section.5.10}% 99
\BOOKMARK [0][-]{section*.3}{Appendices}{}% 100
\BOOKMARK [0][-]{appendix.a.A}{Introduction to probabilities}{}% 101
\BOOKMARK [1][-]{section.a.A.1}{probabilities chain rule}{appendix.a.A}% 102
\BOOKMARK [1][-]{section.a.A.2}{Naive Bayes}{appendix.a.A}% 103
\BOOKMARK [0][-]{appendix.a.B}{Covariance}{}% 104
\BOOKMARK [0][-]{appendix.a.C}{Single Value Decomposition}{}% 105
\BOOKMARK [0][-]{appendix.a.D}{Exponentially weighted averages}{}% 106
\BOOKMARK [1][-]{subsection.a.D.0.1}{How to choose the value \040?}{appendix.a.D}% 107
\BOOKMARK [0][-]{appendix.a.E}{smoothing \(add-k, and add-one laplacian\)}{}% 108
