\BOOKMARK [0][-]{chapter*.2}{Introduction}{}% 1
\BOOKMARK [1][-]{section.0.1}{notation}{chapter*.2}% 2
\BOOKMARK [0][-]{chapter.1}{Logistic Regression as a neural network}{}% 3
\BOOKMARK [1][-]{section.1.1}{definitions}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.2}{cost function}{chapter.1}% 5
\BOOKMARK [1][-]{section.1.3}{Gradient Descent}{chapter.1}% 6
\BOOKMARK [1][-]{section.1.4}{Model training}{chapter.1}% 7
\BOOKMARK [1][-]{section.1.5}{Forward Propagation}{chapter.1}% 8
\BOOKMARK [2][-]{subsection.1.5.1}{Activation Functions}{section.1.5}% 9
\BOOKMARK [1][-]{section.1.6}{Backward Propagation}{chapter.1}% 10
\BOOKMARK [1][-]{section.1.7}{Update parameters}{chapter.1}% 11
\BOOKMARK [1][-]{section.1.8}{Summary}{chapter.1}% 12
\BOOKMARK [1][-]{section.1.9}{Logistic Regression in Python}{chapter.1}% 13
\BOOKMARK [1][-]{section.1.10}{References}{chapter.1}% 14
\BOOKMARK [0][-]{chapter.2}{Neural Networks}{}% 15
\BOOKMARK [1][-]{section.2.1}{Lingua franca}{chapter.2}% 16
\BOOKMARK [1][-]{section.2.2}{Model training}{chapter.2}% 17
\BOOKMARK [1][-]{section.2.3}{Parameter initialization}{chapter.2}% 18
\BOOKMARK [1][-]{section.2.4}{Forward Propagation}{chapter.2}% 19
\BOOKMARK [1][-]{section.2.5}{Backward Propagation}{chapter.2}% 20
\BOOKMARK [1][-]{section.2.6}{Summary}{chapter.2}% 21
\BOOKMARK [1][-]{section.2.7}{Deep neural networks in Python}{chapter.2}% 22
\BOOKMARK [0][-]{chapter.3}{Neural Networks hyperparameters}{}% 23
\BOOKMARK [1][-]{subsection.3.0.1}{training}{chapter.3}% 24
\BOOKMARK [2][-]{subsection.3.0.2}{bias-variance trade-off}{subsection.3.0.1}% 25
\BOOKMARK [2][-]{subsection.3.0.3}{recipes for high-bias, high-variance}{subsection.3.0.1}% 26
\BOOKMARK [2][-]{subsection.3.0.4}{Regularization \(weight decay\)}{subsection.3.0.1}% 27
\BOOKMARK [2][-]{subsection.3.0.5}{Inverted Dropout Regularization}{subsection.3.0.1}% 28
\BOOKMARK [2][-]{subsection.3.0.6}{Input Normalization}{subsection.3.0.1}% 29
\BOOKMARK [2][-]{subsection.3.0.7}{Vanishing/Exploding gradients}{subsection.3.0.1}% 30
\BOOKMARK [2][-]{subsection.3.0.8}{gradient checking}{subsection.3.0.1}% 31
\BOOKMARK [1][-]{section.3.1}{Optimization}{chapter.3}% 32
\BOOKMARK [2][-]{subsection.3.1.1}{stochastic, mini-batch, and batch gradient descent}{section.3.1}% 33
\BOOKMARK [1][-]{section.3.2}{Gradient descent with momentum}{chapter.3}% 34
\BOOKMARK [1][-]{section.3.3}{RMSprob}{chapter.3}% 35
\BOOKMARK [1][-]{section.3.4}{Adaptive Momentum estimation \(Adam\)}{chapter.3}% 36
\BOOKMARK [2][-]{subsection.3.4.1}{Hyperparameters}{section.3.4}% 37
\BOOKMARK [1][-]{section.3.5}{Learning rate decay}{chapter.3}% 38
\BOOKMARK [1][-]{section.3.6}{Tuning parameters}{chapter.3}% 39
\BOOKMARK [2][-]{subsection.3.6.1}{coarse to fine search}{section.3.6}% 40
\BOOKMARK [2][-]{subsection.3.6.2}{panda vs caviar training approaches}{section.3.6}% 41
\BOOKMARK [1][-]{section.3.7}{Batch Normalization}{chapter.3}% 42
\BOOKMARK [2][-]{subsection.3.7.1}{Covariate Shift}{section.3.7}% 43
\BOOKMARK [2][-]{subsection.3.7.2}{Batch normalization as a regularization technique}{section.3.7}% 44
\BOOKMARK [0][-]{chapter.4}{Natural language processing}{}% 45
\BOOKMARK [1][-]{section.4.1}{pre-processing}{chapter.4}% 46
\BOOKMARK [1][-]{section.4.2}{Example: positive, negative classifier}{chapter.4}% 47
\BOOKMARK [1][-]{section.4.3}{Logistic regression classifier}{chapter.4}% 48
\BOOKMARK [1][-]{section.4.4}{Naive Bayes classifier}{chapter.4}% 49
\BOOKMARK [1][-]{section.4.5}{costine similaritis}{chapter.4}% 50
\BOOKMARK [2][-]{subsection.4.5.1}{Euclidean distance}{section.4.5}% 51
\BOOKMARK [1][-]{section.4.6}{Principle Component Analysis \(PCA\)}{chapter.4}% 52
\BOOKMARK [1][-]{section.4.7}{Machine Translation}{chapter.4}% 53
\BOOKMARK [2][-]{subsection.4.7.1}{Loss function L}{section.4.7}% 54
\BOOKMARK [2][-]{subsection.4.7.2}{gradient descent}{section.4.7}% 55
\BOOKMARK [2][-]{subsection.4.7.3}{fixed number of iterations}{section.4.7}% 56
\BOOKMARK [2][-]{subsection.4.7.4}{k-Nearest neighbors algorithm}{section.4.7}% 57
\BOOKMARK [2][-]{subsection.4.7.5}{Searching for the translation embedding}{section.4.7}% 58
\BOOKMARK [2][-]{subsection.4.7.6}{LSH and document search}{section.4.7}% 59
\BOOKMARK [2][-]{subsection.4.7.7}{Bag-of-words \(BOW\) document models}{section.4.7}% 60
\BOOKMARK [2][-]{subsection.4.7.8}{ Choosing the number of planes}{section.4.7}% 61
\BOOKMARK [2][-]{subsection.4.7.9}{ Getting the hash number for a vector}{section.4.7}% 62
\BOOKMARK [1][-]{section.4.8}{Probabilistic model of pronounciation and spelling}{chapter.4}% 63
\BOOKMARK [2][-]{subsection.4.8.1}{auto-correction}{section.4.8}% 64
\BOOKMARK [2][-]{subsection.4.8.2}{Bayesian inference model}{section.4.8}% 65
\BOOKMARK [2][-]{subsection.4.8.3}{Minimum edit distance}{section.4.8}% 66
\BOOKMARK [1][-]{section.4.9}{Grammar Weighted Automata}{chapter.4}% 67
\BOOKMARK [2][-]{subsection.4.9.1}{Markov chain}{section.4.9}% 68
\BOOKMARK [2][-]{subsection.4.9.2}{Hidden Markov Models HMMs}{section.4.9}% 69
\BOOKMARK [2][-]{subsection.4.9.3}{Viterbi Algorithm}{section.4.9}% 70
\BOOKMARK [2][-]{subsection.4.9.4}{Part of Speech tagging \(POS\)}{section.4.9}% 71
\BOOKMARK [1][-]{section.4.10}{N-grams}{chapter.4}% 72
\BOOKMARK [2][-]{subsection.4.10.1}{Smoothing}{section.4.10}% 73
\BOOKMARK [2][-]{subsection.4.10.2}{Back-off}{section.4.10}% 74
\BOOKMARK [2][-]{subsection.4.10.3}{Interpolation}{section.4.10}% 75
\BOOKMARK [0][-]{section*.3}{Appendices}{}% 76
\BOOKMARK [0][-]{appendix.a.A}{Introduction to probabilities}{}% 77
\BOOKMARK [1][-]{section.a.A.1}{probabilities chain rule}{appendix.a.A}% 78
\BOOKMARK [1][-]{section.a.A.2}{Naive Bayes}{appendix.a.A}% 79
\BOOKMARK [0][-]{appendix.a.B}{Covariance}{}% 80
\BOOKMARK [0][-]{appendix.a.C}{Single Value Decomposition}{}% 81
\BOOKMARK [0][-]{appendix.a.D}{Exponentially weighted averages}{}% 82
\BOOKMARK [1][-]{subsection.a.D.0.1}{How to choose the value \040?}{appendix.a.D}% 83
\BOOKMARK [0][-]{appendix.a.E}{smoothing \(add-k, and add-one laplacian\)}{}% 84
