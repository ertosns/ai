\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Introduction}{7}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.1}notation}{7}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Logistic Regression as a neural network}{9}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}definitions}{9}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}cost function}{9}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Gradient Descent}{10}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Model training}{11}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Forward Propagation}{12}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Activation Functions}{12}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Backward Propagation}{12}{section.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Update parameters}{13}{section.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Summary}{13}{section.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Logistic Regression in Python}{13}{section.1.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.10}References}{14}{section.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Neural Networks}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Lingua franca}{15}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Model training}{16}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Parameter initialization}{17}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Forward Propagation}{18}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Backward Propagation}{18}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Summary}{19}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Deep neural networks in Python}{19}{section.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Neural Networks hyperparameters}{21}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.1}training}{21}{subsection.3.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.2}bias-variance trade-off}{22}{subsection.3.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.3}recipes for high-bias, high-variance}{22}{subsection.3.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.4}Regularization (weight decay)}{23}{subsection.3.0.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.5}Inverted Dropout Regularization}{24}{subsection.3.0.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.6}Input Normalization}{24}{subsection.3.0.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.7}Vanishing/Exploding gradients}{24}{subsection.3.0.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.8}gradient checking}{25}{subsection.3.0.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Optimization}{25}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}stochastic, mini-batch, and batch gradient descent}{26}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Gradient descent with momentum}{26}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}RMSprob}{27}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Adaptive Momentum estimation (Adam)}{27}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Hyperparameters}{28}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Learning rate decay}{28}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Tuning parameters}{28}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}coarse to fine search}{29}{subsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}panda vs caviar training approaches}{29}{subsection.3.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Batch Normalization}{29}{section.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Covariate Shift}{30}{subsection.3.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Batch normalization as a regularization technique}{30}{subsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Natural language processing}{31}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}pre-processing}{31}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Example: positive, negative classifier}{31}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Logistic regression classifier}{32}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Naive Bayes classifier}{33}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}costine similaritis}{35}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Euclidean distance}{36}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Principle Component Analysis (PCA)}{36}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Machine Translation}{37}{section.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Loss function L}{38}{subsection.4.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}gradient descent}{39}{subsection.4.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}fixed number of iterations}{40}{subsection.4.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.4}k-Nearest neighbors algorithm}{40}{subsection.4.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.5}Searching for the translation embedding}{41}{subsection.4.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.6}LSH and document search}{41}{subsection.4.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.7}Bag-of-words (BOW) document models}{42}{subsection.4.7.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.8} Choosing the number of planes}{42}{subsection.4.7.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.9} Getting the hash number for a vector}{43}{subsection.4.7.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Probabilistic model of pronounciation and spelling}{44}{section.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}auto-correction}{44}{subsection.4.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Bayesian inference model}{45}{subsection.4.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.3}Minimum edit distance}{46}{subsection.4.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Grammar Weighted Automata}{46}{section.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.1}Markov chain}{46}{subsection.4.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.2}Hidden Markov Models HMMs}{47}{subsection.4.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.3}Viterbi Algorithm}{48}{subsection.4.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.4}Part of Speech tagging (POS)}{49}{subsection.4.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.10}N-grams}{50}{section.4.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.1}Smoothing}{50}{subsection.4.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.2}Back-off}{50}{subsection.4.10.2}\protected@file@percent }
\citation{xyz2}
\bibstyle{apalike}
\bibdata{bib0}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.3}Interpolation}{51}{subsection.4.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Appendices}{53}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Introduction to probabilities}{55}{appendix.a.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}probabilities chain rule}{55}{section.a.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Naive Bayes}{55}{section.a.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Covariance}{57}{appendix.a.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Single Value Decomposition}{59}{appendix.a.C}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {D}Exponentially weighted averages}{61}{appendix.a.D}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.0.1}How to choose the value $\beta $ ?}{61}{subsection.a.D.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {E}smoothing (add-k, and add-one laplacian)}{63}{appendix.a.E}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
