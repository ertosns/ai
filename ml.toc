\contentsline {chapter}{Introduction}{9}{chapter*.2}%
\contentsline {section}{\numberline {0.1}notation}{9}{section.0.1}%
\contentsline {chapter}{\numberline {1}Logistic Regression as a neural network}{11}{chapter.1}%
\contentsline {section}{\numberline {1.1}definitions}{11}{section.1.1}%
\contentsline {section}{\numberline {1.2}cost function}{11}{section.1.2}%
\contentsline {section}{\numberline {1.3}Gradient Descent}{12}{section.1.3}%
\contentsline {section}{\numberline {1.4}Model training}{13}{section.1.4}%
\contentsline {section}{\numberline {1.5}Forward Propagation}{14}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Activation Functions}{14}{subsection.1.5.1}%
\contentsline {section}{\numberline {1.6}Backward Propagation}{14}{section.1.6}%
\contentsline {section}{\numberline {1.7}Update parameters}{15}{section.1.7}%
\contentsline {section}{\numberline {1.8}Summary}{15}{section.1.8}%
\contentsline {section}{\numberline {1.9}Logistic Regression in Python}{15}{section.1.9}%
\contentsline {section}{\numberline {1.10}References}{16}{section.1.10}%
\contentsline {chapter}{\numberline {2}Neural Networks}{17}{chapter.2}%
\contentsline {section}{\numberline {2.1}Lingua franca}{17}{section.2.1}%
\contentsline {section}{\numberline {2.2}Model training}{18}{section.2.2}%
\contentsline {section}{\numberline {2.3}Parameter initialization}{19}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Xavier initialization}{20}{subsection.2.3.1}%
\contentsline {section}{\numberline {2.4}Forward Propagation}{20}{section.2.4}%
\contentsline {section}{\numberline {2.5}Backward Propagation}{20}{section.2.5}%
\contentsline {section}{\numberline {2.6}Summary}{21}{section.2.6}%
\contentsline {section}{\numberline {2.7}Deep neural networks in Python}{21}{section.2.7}%
\contentsline {chapter}{\numberline {3}Neural Networks hyperparameters}{23}{chapter.3}%
\contentsline {subsection}{\numberline {3.0.1}training}{23}{subsection.3.0.1}%
\contentsline {subsection}{\numberline {3.0.2}bias-variance trade-off}{24}{subsection.3.0.2}%
\contentsline {subsection}{\numberline {3.0.3}recipes for high-bias, high-variance}{24}{subsection.3.0.3}%
\contentsline {subsection}{\numberline {3.0.4}Regularization (weight decay)}{25}{subsection.3.0.4}%
\contentsline {subsection}{\numberline {3.0.5}Inverted Dropout Regularization}{26}{subsection.3.0.5}%
\contentsline {subsection}{\numberline {3.0.6}Input Normalization}{26}{subsection.3.0.6}%
\contentsline {subsection}{\numberline {3.0.7}Vanishing/Exploding gradients}{26}{subsection.3.0.7}%
\contentsline {subsection}{\numberline {3.0.8}gradient checking}{27}{subsection.3.0.8}%
\contentsline {section}{\numberline {3.1}Optimization}{27}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}stochastic, mini-batch, and batch gradient descent}{28}{subsection.3.1.1}%
\contentsline {section}{\numberline {3.2}Gradient descent with momentum}{28}{section.3.2}%
\contentsline {section}{\numberline {3.3}RMSprob}{29}{section.3.3}%
\contentsline {section}{\numberline {3.4}Adaptive Momentum estimation (Adam)}{29}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Hyperparameters}{30}{subsection.3.4.1}%
\contentsline {section}{\numberline {3.5}Learning rate decay}{30}{section.3.5}%
\contentsline {section}{\numberline {3.6}Tuning parameters}{30}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}coarse to fine search}{31}{subsection.3.6.1}%
\contentsline {subsection}{\numberline {3.6.2}panda vs caviar training approaches}{31}{subsection.3.6.2}%
\contentsline {section}{\numberline {3.7}Batch Normalization}{31}{section.3.7}%
\contentsline {subsection}{\numberline {3.7.1}Covariate Shift}{32}{subsection.3.7.1}%
\contentsline {subsection}{\numberline {3.7.2}Batch normalization as a regularization technique}{32}{subsection.3.7.2}%
\contentsline {subsection}{\numberline {3.7.3}Batch normalization on test sets}{33}{subsection.3.7.3}%
\contentsline {section}{\numberline {3.8}Multi-class classification}{33}{section.3.8}%
\contentsline {subsection}{\numberline {3.8.1}Softmax activation}{33}{subsection.3.8.1}%
\contentsline {chapter}{\numberline {4}Structuring machine learning}{35}{chapter.4}%
\contentsline {section}{\numberline {4.1}Machine learning strategy}{35}{section.4.1}%
\contentsline {section}{\numberline {4.2}orthogonality}{35}{section.4.2}%
\contentsline {section}{\numberline {4.3}Set up your Goal}{36}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Evaluation metric}{36}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Precision vs accuracy}{36}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}F1-score}{37}{subsection.4.3.3}%
\contentsline {subsection}{\numberline {4.3.4}Satisfying-Optimizing metric}{37}{subsection.4.3.4}%
\contentsline {section}{\numberline {4.4}train/dev/test sets}{37}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}dev/test metric}{38}{subsection.4.4.1}%
\contentsline {section}{\numberline {4.5}Human-level performance}{38}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}human(bayes) error}{38}{subsection.4.5.1}%
\contentsline {section}{\numberline {4.6}Error analysis}{39}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Mislabeled data}{40}{subsection.4.6.1}%
\contentsline {section}{\numberline {4.7}Mismatched training dev/test sets}{40}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Addressing data mismatch}{41}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Artificial data synthesis}{41}{subsection.4.7.2}%
\contentsline {section}{\numberline {4.8}Multiple tasks learning}{42}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Transfer learning}{42}{subsection.4.8.1}%
\contentsline {subsection}{\numberline {4.8.2}Multitask learning}{42}{subsection.4.8.2}%
\contentsline {section}{\numberline {4.9}End-to-End Machine Learning}{42}{section.4.9}%
\contentsline {chapter}{\numberline {5}Computer Vision}{43}{chapter.5}%
\contentsline {section}{\numberline {5.1}Object Detection}{43}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Edge Detection}{44}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Padding}{45}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {5.1.3}striding}{46}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Convolution on RGB channels}{46}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Multiple filters Convolution}{46}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6}Example ConvNet}{46}{subsection.5.1.6}%
\contentsline {subsection}{\numberline {5.1.7}Pooling Convolutions}{47}{subsection.5.1.7}%
\contentsline {subsection}{\numberline {5.1.8}Max Pooling}{47}{subsection.5.1.8}%
\contentsline {subsection}{\numberline {5.1.9}Average Pooling}{48}{subsection.5.1.9}%
\contentsline {section}{\numberline {5.2}Examples}{48}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}LeNet-5 Network}{48}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}AlexNet Network}{48}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}VGG-16 Network}{49}{subsection.5.2.3}%
\contentsline {subsection}{\numberline {5.2.4}ResNet (Residual Block)}{50}{subsection.5.2.4}%
\contentsline {section}{\numberline {5.3}Inception}{50}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}$1\times {1}$ Convolutions}{51}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Inception Block}{51}{subsection.5.3.2}%
\contentsline {subsection}{\numberline {5.3.3}Inception Branches}{51}{subsection.5.3.3}%
\contentsline {section}{\numberline {5.4}Object Detection}{51}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Localization and Detection}{51}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}classification with localization}{52}{subsection.5.4.2}%
\contentsline {subsection}{\numberline {5.4.3}Landmark detection}{52}{subsection.5.4.3}%
\contentsline {subsection}{\numberline {5.4.4}Object Detection}{53}{subsection.5.4.4}%
\contentsline {subsection}{\numberline {5.4.5}Convolutional sliding windows}{53}{subsection.5.4.5}%
\contentsline {subsection}{\numberline {5.4.6}Example:}{53}{subsection.5.4.6}%
\contentsline {section}{\numberline {5.5}Bounding Box prediction}{54}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}YOLO}{54}{subsection.5.5.1}%
\contentsline {subsection}{\numberline {5.5.2}Intersection over Union IOU function}{54}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}Non-Max Suppression}{55}{subsection.5.5.3}%
\contentsline {subsection}{\numberline {5.5.4}Anchor Boxes}{55}{subsection.5.5.4}%
\contentsline {subsection}{\numberline {5.5.5}Region Proposal R-CNN, Fast R-CNN}{55}{subsection.5.5.5}%
\contentsline {section}{\numberline {5.6}Face Recognition }{56}{section.5.6}%
\contentsline {subsection}{\numberline {5.6.1}Recognition vs. Verification}{56}{subsection.5.6.1}%
\contentsline {subsection}{\numberline {5.6.2}One-Shot learning}{56}{subsection.5.6.2}%
\contentsline {subsection}{\numberline {5.6.3}Similarity learning function}{56}{subsection.5.6.3}%
\contentsline {subsection}{\numberline {5.6.4}Siamese Network}{57}{subsection.5.6.4}%
\contentsline {subsection}{\numberline {5.6.5}Triplet Loss}{57}{subsection.5.6.5}%
\contentsline {subsection}{\numberline {5.6.6}Binary Classification}{57}{subsection.5.6.6}%
\contentsline {section}{\numberline {5.7}Neural Style Transfer}{58}{section.5.7}%
\contentsline {subsection}{\numberline {5.7.1}Generated image descent}{58}{subsection.5.7.1}%
\contentsline {subsection}{\numberline {5.7.2}Content cost}{59}{subsection.5.7.2}%
\contentsline {subsection}{\numberline {5.7.3}Style cost}{59}{subsection.5.7.3}%
\contentsline {subsection}{\numberline {5.7.4}Gram Matrix(style)}{59}{subsection.5.7.4}%
\contentsline {chapter}{\numberline {6}Recurrent Neural Networks RNN}{61}{chapter.6}%
\contentsline {section}{\numberline {6.1}Introduction}{61}{section.6.1}%
\contentsline {section}{\numberline {6.2}Notation}{61}{section.6.2}%
\contentsline {section}{\numberline {6.3}RNN}{62}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}RNN model}{62}{subsection.6.3.1}%
\contentsline {section}{\numberline {6.4}uni-directional RNN Forward propagation}{62}{section.6.4}%
\contentsline {section}{\numberline {6.5}uni-directional RNN backward propagation through time}{63}{section.6.5}%
\contentsline {section}{\numberline {6.6}Variations of RNN models}{63}{section.6.6}%
\contentsline {subsection}{\numberline {6.6.1}many-to-one RNN}{64}{subsection.6.6.1}%
\contentsline {subsection}{\numberline {6.6.2}One-to-Many RNN}{64}{subsection.6.6.2}%
\contentsline {subsection}{\numberline {6.6.3}Many-to-Many of different input/output length}{66}{subsection.6.6.3}%
\contentsline {section}{\numberline {6.7}Language Model and sequence generation}{66}{section.6.7}%
\contentsline {section}{\numberline {6.8}Sampling novel sequences}{67}{section.6.8}%
\contentsline {section}{\numberline {6.9}Vanishing/Exploding gradients}{67}{section.6.9}%
\contentsline {subsection}{\numberline {6.9.1}Gated Recurrent Unit (GRU)}{67}{subsection.6.9.1}%
\contentsline {subsection}{\numberline {6.9.2}GRU simplified}{68}{subsection.6.9.2}%
\contentsline {subsection}{\numberline {6.9.3}Long Short Term Memory (LSTM)}{68}{subsection.6.9.3}%
\contentsline {subsection}{\numberline {6.9.4}Back-propagation}{69}{subsection.6.9.4}%
\contentsline {subsection}{\numberline {6.9.5}Bidirectional RNN}{69}{subsection.6.9.5}%
\contentsline {section}{\numberline {6.10}Deep RNNs}{70}{section.6.10}%
\contentsline {section}{\numberline {6.11}Word Representation}{70}{section.6.11}%
\contentsline {subsection}{\numberline {6.11.1}Word Embedding}{70}{subsection.6.11.1}%
\contentsline {subsection}{\numberline {6.11.2}Named Entity recognition}{70}{subsection.6.11.2}%
\contentsline {subsection}{\numberline {6.11.3}Learning word embedding}{71}{subsection.6.11.3}%
\contentsline {subsection}{\numberline {6.11.4}Word2Vec}{71}{subsection.6.11.4}%
\contentsline {chapter}{\numberline {7}Probabilistic Graphical Models (PGM)}{73}{chapter.7}%
\contentsline {section}{\numberline {7.1}Introduction}{73}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}preliminaries}{74}{subsection.7.1.1}%
\contentsline {subsection}{\numberline {7.1.2}factors}{74}{subsection.7.1.2}%
\contentsline {section}{\numberline {7.2}Bayesian Network Fundamentals}{75}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}constructing dependencies}{75}{subsection.7.2.1}%
\contentsline {section}{\numberline {7.3}Reasoning Patterns on BN}{76}{section.7.3}%
\contentsline {section}{\numberline {7.4}Flow of Probabilistic Influence}{77}{section.7.4}%
\contentsline {section}{\numberline {7.5}d-separation}{77}{section.7.5}%
\contentsline {subsection}{\numberline {7.5.1}I-map}{78}{subsection.7.5.1}%
\contentsline {section}{\numberline {7.6}Naive Bayes Model}{78}{section.7.6}%
\contentsline {section}{\numberline {7.7}Template Model}{79}{section.7.7}%
\contentsline {subsection}{\numberline {7.7.1}Distribution over Trajectories}{80}{subsection.7.7.1}%
\contentsline {subsection}{\numberline {7.7.2}Markov chain}{80}{subsection.7.7.2}%
\contentsline {subsection}{\numberline {7.7.3}Dynamic Bayesian Network DBN}{81}{subsection.7.7.3}%
\contentsline {subsection}{\numberline {7.7.4}Hidden Markov Model (HMM)}{81}{subsection.7.7.4}%
\contentsline {section}{\numberline {7.8}Structured CPD}{81}{section.7.8}%
\contentsline {subsection}{\numberline {7.8.1}Tree CPD}{82}{subsection.7.8.1}%
\contentsline {subsection}{\numberline {7.8.2}Multiplexer CPD}{83}{subsection.7.8.2}%
\contentsline {chapter}{\numberline {8}Natural language processing}{85}{chapter.8}%
\contentsline {section}{\numberline {8.1}pre-processing}{85}{section.8.1}%
\contentsline {section}{\numberline {8.2}Example: positive, negative classifier}{85}{section.8.2}%
\contentsline {section}{\numberline {8.3}Logistic regression classifier}{86}{section.8.3}%
\contentsline {section}{\numberline {8.4}Naive Bayes classifier}{87}{section.8.4}%
\contentsline {section}{\numberline {8.5}cosine similaritis}{89}{section.8.5}%
\contentsline {subsection}{\numberline {8.5.1}Euclidean distance}{90}{subsection.8.5.1}%
\contentsline {section}{\numberline {8.6}Principle Component Analysis (PCA)}{90}{section.8.6}%
\contentsline {section}{\numberline {8.7}Machine Translation}{91}{section.8.7}%
\contentsline {subsection}{\numberline {8.7.1}Loss function L}{92}{subsection.8.7.1}%
\contentsline {subsection}{\numberline {8.7.2}gradient descent}{93}{subsection.8.7.2}%
\contentsline {subsection}{\numberline {8.7.3}fixed number of iterations}{94}{subsection.8.7.3}%
\contentsline {subsection}{\numberline {8.7.4}k-Nearest neighbors algorithm}{94}{subsection.8.7.4}%
\contentsline {subsection}{\numberline {8.7.5}Searching for the translation embedding}{95}{subsection.8.7.5}%
\contentsline {subsection}{\numberline {8.7.6}LSH and document search}{95}{subsection.8.7.6}%
\contentsline {subsection}{\numberline {8.7.7}Bag-of-words (BOW) document models}{96}{subsection.8.7.7}%
\contentsline {subsection}{\numberline {8.7.8} Choosing the number of planes}{96}{subsection.8.7.8}%
\contentsline {subsection}{\numberline {8.7.9} Getting the hash number for a vector}{97}{subsection.8.7.9}%
\contentsline {section}{\numberline {8.8}Probabilistic model of pronounciation and spelling}{98}{section.8.8}%
\contentsline {subsection}{\numberline {8.8.1}auto-correction}{98}{subsection.8.8.1}%
\contentsline {subsection}{\numberline {8.8.2}Bayesian inference model}{99}{subsection.8.8.2}%
\contentsline {subsection}{\numberline {8.8.3}Minimum edit distance}{100}{subsection.8.8.3}%
\contentsline {section}{\numberline {8.9}Grammar Weighted Automata}{100}{section.8.9}%
\contentsline {subsection}{\numberline {8.9.1}Markov chain}{100}{subsection.8.9.1}%
\contentsline {subsection}{\numberline {8.9.2}Hidden Markov Models HMMs}{101}{subsection.8.9.2}%
\contentsline {subsection}{\numberline {8.9.3}Viterbi Algorithm}{102}{subsection.8.9.3}%
\contentsline {subsection}{\numberline {8.9.4}Part of Speech tagging (POS)}{103}{subsection.8.9.4}%
\contentsline {section}{\numberline {8.10}N-grams}{104}{section.8.10}%
\contentsline {subsection}{\numberline {8.10.1}Smoothing}{104}{subsection.8.10.1}%
\contentsline {subsection}{\numberline {8.10.2}Back-off}{104}{subsection.8.10.2}%
\contentsline {subsection}{\numberline {8.10.3}Interpolation}{105}{subsection.8.10.3}%
\contentsline {chapter}{Appendices}{107}{section*.3}%
\contentsline {chapter}{\numberline {A}Introduction to probabilities}{109}{appendix.a.A}%
\contentsline {section}{\numberline {A.1}probabilities chain rule}{109}{section.a.A.1}%
\contentsline {section}{\numberline {A.2}Naive Bayes}{109}{section.a.A.2}%
\contentsline {chapter}{\numberline {B}Covariance}{111}{appendix.a.B}%
\contentsline {chapter}{\numberline {C}Single Value Decomposition}{113}{appendix.a.C}%
\contentsline {chapter}{\numberline {D}Exponentially weighted averages}{115}{appendix.a.D}%
\contentsline {subsection}{\numberline {D.0.1}How to choose the value $\beta $ ?}{115}{subsection.a.D.0.1}%
\contentsline {chapter}{\numberline {E}smoothing (add-k, and add-one Laplacian)}{117}{appendix.a.E}%
\contentsline {chapter}{\numberline {F}Kernels, and Convolution functions}{119}{appendix.a.F}%
