\contentsline {chapter}{Introduction}{7}{chapter*.2}%
\contentsline {section}{\numberline {0.1}notation}{7}{section.0.1}%
\contentsline {chapter}{\numberline {1}Logistic Regression as a neural network}{9}{chapter.1}%
\contentsline {section}{\numberline {1.1}definitions}{9}{section.1.1}%
\contentsline {section}{\numberline {1.2}cost function}{9}{section.1.2}%
\contentsline {section}{\numberline {1.3}Gradient Descent}{10}{section.1.3}%
\contentsline {section}{\numberline {1.4}Model training}{11}{section.1.4}%
\contentsline {section}{\numberline {1.5}Forward Propagation}{12}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Activation Functions}{12}{subsection.1.5.1}%
\contentsline {section}{\numberline {1.6}Backward Propagation}{12}{section.1.6}%
\contentsline {section}{\numberline {1.7}Update parameters}{13}{section.1.7}%
\contentsline {section}{\numberline {1.8}Summary}{13}{section.1.8}%
\contentsline {section}{\numberline {1.9}Logistic Regression in Python}{13}{section.1.9}%
\contentsline {section}{\numberline {1.10}References}{14}{section.1.10}%
\contentsline {chapter}{\numberline {2}Neural Networks}{15}{chapter.2}%
\contentsline {section}{\numberline {2.1}Lingua franca}{15}{section.2.1}%
\contentsline {section}{\numberline {2.2}Model training}{16}{section.2.2}%
\contentsline {section}{\numberline {2.3}Parameter initialization}{17}{section.2.3}%
\contentsline {section}{\numberline {2.4}Forward Propagation}{18}{section.2.4}%
\contentsline {section}{\numberline {2.5}Backward Propagation}{18}{section.2.5}%
\contentsline {section}{\numberline {2.6}Summary}{19}{section.2.6}%
\contentsline {section}{\numberline {2.7}Deep neural networks in Python}{19}{section.2.7}%
\contentsline {chapter}{\numberline {3}Neural Networks hyperparameters}{21}{chapter.3}%
\contentsline {subsection}{\numberline {3.0.1}training}{21}{subsection.3.0.1}%
\contentsline {subsection}{\numberline {3.0.2}bias-variance trade-off}{22}{subsection.3.0.2}%
\contentsline {subsection}{\numberline {3.0.3}recipes for high-bias, high-variance}{22}{subsection.3.0.3}%
\contentsline {subsection}{\numberline {3.0.4}Regularization (weight decay)}{23}{subsection.3.0.4}%
\contentsline {subsection}{\numberline {3.0.5}Inverted Dropout Regularization}{24}{subsection.3.0.5}%
\contentsline {subsection}{\numberline {3.0.6}Input Normalization}{24}{subsection.3.0.6}%
\contentsline {subsection}{\numberline {3.0.7}Vanishing/Exploding gradients}{24}{subsection.3.0.7}%
\contentsline {subsection}{\numberline {3.0.8}gradient checking}{25}{subsection.3.0.8}%
\contentsline {section}{\numberline {3.1}Optimization}{25}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}stochastic, mini-batch, and batch gradient descent}{26}{subsection.3.1.1}%
\contentsline {section}{\numberline {3.2}Gradient descent with momentum}{26}{section.3.2}%
\contentsline {section}{\numberline {3.3}RMSprob}{27}{section.3.3}%
\contentsline {section}{\numberline {3.4}Adaptive Momentum estimation (Adam)}{27}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Hyperparameters}{28}{subsection.3.4.1}%
\contentsline {section}{\numberline {3.5}Learning rate decay}{28}{section.3.5}%
\contentsline {section}{\numberline {3.6}Tuning parameters}{28}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}coarse to fine search}{29}{subsection.3.6.1}%
\contentsline {subsection}{\numberline {3.6.2}panda vs caviar training approaches}{29}{subsection.3.6.2}%
\contentsline {section}{\numberline {3.7}Batch Normalization}{29}{section.3.7}%
\contentsline {subsection}{\numberline {3.7.1}Covariate Shift}{30}{subsection.3.7.1}%
\contentsline {subsection}{\numberline {3.7.2}Batch normalization as a regularization technique}{30}{subsection.3.7.2}%
\contentsline {subsection}{\numberline {3.7.3}Batch normalization on test sets}{31}{subsection.3.7.3}%
\contentsline {section}{\numberline {3.8}Multi-class classification}{31}{section.3.8}%
\contentsline {subsection}{\numberline {3.8.1}Softmax activation}{31}{subsection.3.8.1}%
\contentsline {chapter}{\numberline {4}Structuring machine learning}{33}{chapter.4}%
\contentsline {section}{\numberline {4.1}Machine learning strategy}{33}{section.4.1}%
\contentsline {section}{\numberline {4.2}orthogonality}{33}{section.4.2}%
\contentsline {section}{\numberline {4.3}Set up your Goal}{34}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Evaluation metric}{34}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Precision vs accuracy}{34}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}F1-score}{35}{subsection.4.3.3}%
\contentsline {subsection}{\numberline {4.3.4}Satisfying-Optimizing metric}{35}{subsection.4.3.4}%
\contentsline {section}{\numberline {4.4}train/dev/test sets}{35}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}dev/test metric}{36}{subsection.4.4.1}%
\contentsline {section}{\numberline {4.5}Human-level performance}{36}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}human(bayes) error}{36}{subsection.4.5.1}%
\contentsline {section}{\numberline {4.6}Error analysis}{37}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Mislabeled data}{38}{subsection.4.6.1}%
\contentsline {section}{\numberline {4.7}Mismatched training dev/test sets}{38}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Addressing data mismatch}{39}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Artificial data synthesis}{39}{subsection.4.7.2}%
\contentsline {section}{\numberline {4.8}Multiple tasks learning}{40}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Transfer learning}{40}{subsection.4.8.1}%
\contentsline {subsection}{\numberline {4.8.2}Multitask learning}{40}{subsection.4.8.2}%
\contentsline {section}{\numberline {4.9}End-to-End Machine Learning}{40}{section.4.9}%
\contentsline {chapter}{\numberline {5}Computer Vision}{41}{chapter.5}%
\contentsline {section}{\numberline {5.1}Object Detection}{41}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Edge Detection}{42}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Padding}{43}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {5.1.3}striding}{44}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Convolution on RGB channels}{44}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Multiple filters Convolution}{44}{subsection.5.1.5}%
\contentsline {chapter}{\numberline {6}Natural language processing}{45}{chapter.6}%
\contentsline {section}{\numberline {6.1}pre-processing}{45}{section.6.1}%
\contentsline {section}{\numberline {6.2}Example: positive, negative classifier}{45}{section.6.2}%
\contentsline {section}{\numberline {6.3}Logistic regression classifier}{46}{section.6.3}%
\contentsline {section}{\numberline {6.4}Naive Bayes classifier}{47}{section.6.4}%
\contentsline {section}{\numberline {6.5}costine similaritis}{49}{section.6.5}%
\contentsline {subsection}{\numberline {6.5.1}Euclidean distance}{50}{subsection.6.5.1}%
\contentsline {section}{\numberline {6.6}Principle Component Analysis (PCA)}{50}{section.6.6}%
\contentsline {section}{\numberline {6.7}Machine Translation}{51}{section.6.7}%
\contentsline {subsection}{\numberline {6.7.1}Loss function L}{52}{subsection.6.7.1}%
\contentsline {subsection}{\numberline {6.7.2}gradient descent}{53}{subsection.6.7.2}%
\contentsline {subsection}{\numberline {6.7.3}fixed number of iterations}{54}{subsection.6.7.3}%
\contentsline {subsection}{\numberline {6.7.4}k-Nearest neighbors algorithm}{54}{subsection.6.7.4}%
\contentsline {subsection}{\numberline {6.7.5}Searching for the translation embedding}{55}{subsection.6.7.5}%
\contentsline {subsection}{\numberline {6.7.6}LSH and document search}{55}{subsection.6.7.6}%
\contentsline {subsection}{\numberline {6.7.7}Bag-of-words (BOW) document models}{56}{subsection.6.7.7}%
\contentsline {subsection}{\numberline {6.7.8} Choosing the number of planes}{56}{subsection.6.7.8}%
\contentsline {subsection}{\numberline {6.7.9} Getting the hash number for a vector}{57}{subsection.6.7.9}%
\contentsline {section}{\numberline {6.8}Probabilistic model of pronounciation and spelling}{58}{section.6.8}%
\contentsline {subsection}{\numberline {6.8.1}auto-correction}{58}{subsection.6.8.1}%
\contentsline {subsection}{\numberline {6.8.2}Bayesian inference model}{59}{subsection.6.8.2}%
\contentsline {subsection}{\numberline {6.8.3}Minimum edit distance}{60}{subsection.6.8.3}%
\contentsline {section}{\numberline {6.9}Grammar Weighted Automata}{60}{section.6.9}%
\contentsline {subsection}{\numberline {6.9.1}Markov chain}{60}{subsection.6.9.1}%
\contentsline {subsection}{\numberline {6.9.2}Hidden Markov Models HMMs}{61}{subsection.6.9.2}%
\contentsline {subsection}{\numberline {6.9.3}Viterbi Algorithm}{62}{subsection.6.9.3}%
\contentsline {subsection}{\numberline {6.9.4}Part of Speech tagging (POS)}{63}{subsection.6.9.4}%
\contentsline {section}{\numberline {6.10}N-grams}{64}{section.6.10}%
\contentsline {subsection}{\numberline {6.10.1}Smoothing}{64}{subsection.6.10.1}%
\contentsline {subsection}{\numberline {6.10.2}Back-off}{64}{subsection.6.10.2}%
\contentsline {subsection}{\numberline {6.10.3}Interpolation}{65}{subsection.6.10.3}%
\contentsline {chapter}{Appendices}{67}{section*.3}%
\contentsline {chapter}{\numberline {A}Introduction to probabilities}{69}{appendix.a.A}%
\contentsline {section}{\numberline {A.1}probabilities chain rule}{69}{section.a.A.1}%
\contentsline {section}{\numberline {A.2}Naive Bayes}{69}{section.a.A.2}%
\contentsline {chapter}{\numberline {B}Covariance}{71}{appendix.a.B}%
\contentsline {chapter}{\numberline {C}Single Value Decomposition}{73}{appendix.a.C}%
\contentsline {chapter}{\numberline {D}Exponentially weighted averages}{75}{appendix.a.D}%
\contentsline {subsection}{\numberline {D.0.1}How to choose the value $\beta $ ?}{75}{subsection.a.D.0.1}%
\contentsline {chapter}{\numberline {E}smoothing (add-k, and add-one Laplacian)}{77}{appendix.a.E}%
\contentsline {chapter}{\numberline {F}Kernels, and Convolution functions}{79}{appendix.a.F}%
